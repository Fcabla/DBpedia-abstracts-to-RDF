# DBpedia abstracts to RDF

## Abstract
This section of the repository comprises the use of generative language tools, such as seq2seq, to extract the possible relationships existing in a document.
In particular, we have tested the use of the REBEL tool, which uses a generative autoregressive seq2seq model.

## 1. Introduction

## 2. Related work

### 2.1 Relation Extraction methods
[REBEL][1]: Relation Extraction By End-to-end Language generation 20 29 by Cabot and Navigli, presented at EMNLP 2021.

The tool that will be mainly explored in this work to extract the relationships between the entities of the same text. It is achieved by using a seq2seq model based on BART.

[GenerativeRE][2]: Incorporating a Novel Copy Mechanism and Pretrained Model for Joint Entity and Relation Extraction 4 by Jiarun Cao, Sophia Ananiadou at EMNLP 2021.

The authors claim that seq2seq models can suffer from incompletion and disorder problems when they extract multi-token entities from input sentences. They propose to use BIO tags before the input representation to improve generation. The model they use is based on transformers. 

[GenIE][3]: Generative Information Extraction 4 by Martin Josifoski, Nicola De Cao, Maxime Peyrard, Robert West.

Another work presenting an end-to-end solution for the extraction of information from a text document using generative methods. 
GenIE uses the BART architecture based on transformers.
To train this model they use the dataset generated by a tool presented in the REBEL paper. This work would be interesting to test and compare with REBEL.

[doc2rel][6]: Giorgi, J., Bader, G. D., & Wang, B. (2022). A sequence-to-sequence approach for document-level relation extraction. arXiv preprint arXiv:2204.01098.

Similar to REBEL, it extends the idea of using seq2seq as a generative model. It is highly specialized in the medical language, so it is not so interesting for our problem.

[GITHUB_REPO][7]: Collection of papers about relation extraction

### 2.2 REBEL aplications

### 2.3 Other resources
[DBpedia forum][4]: Gsoc proyect ideas and proposals

[REBEL repo][5]: github repository

[1]: https://aclanthology.org/2021.findings-emnlp.204.pdf
[2]: https://aclanthology.org/2021.findings-emnlp.182.pdf
[3]: https://arxiv.org/pdf/2112.08340.pdf
[4]: https://forum.dbpedia.org/t/lg2rdf-language-generation-to-generate-rdf-from-dbpedia-abstracts-gsoc2022/1545
[5]: https://github.com/Babelscape/rebel
[6]: https://arxiv.org/pdf/2204.01098.pdf
[7]: https://github.com/roomylee/awesome-relation-extraction

## 3. Approach
Depending on the operations performed before performing inference with the REBEL model, a somewhat different set of results is obtained. In fact, when using the same model from different sources (spacy, hugging faces and transformers) different results can be obtained. After discussing this with the author of the [repository](https://github.com/Babelscape/rebel/issues/41) he confirmed that the sequence of steps up to the inference has a considerable impact on the result.

One of the steps that can be applied and that generate different results is the use of correferences, where the main idea is to replace those terms that refer to an entity by the name of this entity, an example would be the following: 'Although everyone saw the President, no one recognized him'.

Some of the hyperparameters used by the REBEL model are the following: 
- "max_length": 
- "length_penalty": increasing this attribute generates more triples but may increase the model's hallucination.
- "num_beams": mainly affects the quality of the triples but is associated with a higher computational cost.
- "num_return_sequences": 

Another preprocessing that could improve the quality of the output would be to establish an efficient method of splitting long text documents into small chunks or sentences before passing it to the model.

The pipeline consists of a first step where the text document to be translated is read and a series of text processing operations are applied.
Subsequently, the model is fed with the result of the first step of the pipeline and a set of text triples is obtained. These will be given by a subject, a relation and an object with the following format: 'subject|relation|object'.

The next step is to translate the subject and the object of the text triplet into RDF format. To do this, first the DBpedia Spotlight tool will be used to extract the names of the entities recognized in the entire text document. In this way a dictionary with the textual names and their equivalent URIs is obtained, which is used to replace the first and last element of the text triples.

The last step consists of translating the text properties into RDF format. For this we have found no other way than to create a lexicalization, where we have expressly defined what is the RDF equivalent of each type of text property produced by the REBEL model. This last step is far from perfect, as there may be cases in which triples are generated where the domain and range of the relation differ from the class of the subject and object.

The object and subject are lexicalized using DBpedia Spotlight. We have found that feeding Spotlight with text triples improves the identification of entities in the sense that they are more closely matched to the elements of the triples.
## 4. Evaluation
Compare the use of REBEL together with DBpedia Spotlight to the performance we obtained in Gsoc's project and Pablo's TFM.

Since evaluating the quality of the generated triples is a manual and somewhat complicated job, for the moment we will look at the amount of text triples obtained at the end of the pipeline and the amount of RDF triples we can extract from the output generated by the model.

It would be interesting to find a dataset that would be useful for the evaluation of our proposal, so that for each abstract there is a set of triples. Then, we could use our approach to translate that abstract and compare the triples in the dataset with the ones generated by us. We would have to consider the number of triples that match, those that have not been generated and those that have been generated that were not in the dataset.

For now we count how many text triplets are generated, how many are transformed into RDF triples. How many generated RDF triplets are alredy in the DBpedia graph. Also, if the property range and domain of each triplet matches the type of the subject and object.

## 5. Results
In the next plot there is a comparison between Text2RDF and REBEL. 
![Text2RDF vs REBEL avg of triplets generated per text document](https://raw.githubusercontent.com/Fcabla/DBpedia-abstracts-to-RDF/main/Rebel/results/text2rdf_rebel.svg)

Rebel can generate much more triplets than Text2RDF. Also, the rate of conversion between text triplets and RDF triplets is higher for REBEL.

When lexicalizing, we may encounter several cases. These are shown in the following image.

![Different cases that can occurr while lexicalizating](https://raw.githubusercontent.com/Fcabla/DBpedia-abstracts-to-RDF/main/Rebel/results/lexicalization_examples.svg)

The ideal case would be case A, where the whole element of the triplet (Barack Hussein Obama II) has been identified and fully subcategorized by DBpedia Spotlight, so the lexicalization is "perfect".

On the contrary, the worst case is case B, where no lexicalization is found for a certain element of the triplet. In the figure it can be seen how the element 'Lefkogeia' has not been identified and sub-categorized.

Case C occurs when in an element of the triplet a lexicalization has been found that does not fit the whole text that forms that element. In the figure it can be seen that of the element 'Mann Made Hits' only 'Mann Made' has been identified, so the lexicalization is imperfect.

The last case, labeled with the letter D, is very similar to case C. Where we have an imperfect lexicalization but by looking at the URIs and normalizing them we can see that it fits the element perfectly. For example, in the figure above we can see that 'President' has been identified as an entity, but the rest of the element of the triplet is not part of this entity 'of the United States'. However, if we look at the URI of the identified entity we can see that it corresponds to the element 'dbpedia.org/resource/President_of_the_United_States'. Taking advantage of this we can complete the lexicalization.

We have explored several lexicalization strategies for REBEL. Those are shown in the following plot.
![Percentage of RDF triples generated with each lexicalization strategy when using REBEL.](https://raw.githubusercontent.com/Fcabla/DBpedia-abstracts-to-RDF/main/Rebel/results/lexicalization_strategy_rebel.svg)

First we consider that if any lexicalization (perfect or imperfect) is found in an element of the triplet, it is considered valid by choosing a random one (Rebel_any). This would correspond to cases A, C and D in the previous figure.
Obviously this is a bad idea as it is very likely that many errors will be inserted in the graph. However, this gives us an idea of the maximum number of triples that could be lexicalized since the rest would correspond to case B, i.e. DBpedia Spotlight has not been able to associate them to any resource of the ontology.

The second bar, Rebel_exact, would correspond to case A exclusively. Only those elements that are fully identified will be lexicalized and considered as good.

The third strategy corresponds to cases A and D, where if there is no perfect lexicalization we try to find it in the URIs of the fragments identified by Spotlight. This count is very useful as it allows us to check the % of extra triples that we can translate if we take into account case D. Being 3% more than if we only count as correct those of case A, it is not much.

Finally, the Rebel_othertypes strategy is the same as the previous one but taking into account that there are some properties that expect literals as objects. The % of cases covered taking this into account is 2%.

We can observe that case B occurs 30% of the time, i.e. a considerable part of our triples is not being lexicalized perfectly, but candidates are found that do not cover the whole element.

To get a better idea of what happens during lexicalization we have counted how many times each case occurs. The difference between this count and the information we can conclude from the previous graphs is that in this case, the count will be per triplet element, rather than per triplet. This means that in a triplet we can find case A in the subject and case C in the object, considering this triplet as erroneous (because not all the elements have been lexicalized correctly).

La distribucion de casos se muestra en el siguiente plot.
![Distribution of the different cases.](https://raw.githubusercontent.com/Fcabla/DBpedia-abstracts-to-RDF/main/Rebel/results/case_distribution_piel.svg)

As can be seen, two more cases have been considered. Case E occurs when the property is not lexicalized correctly due to an error that exists in the code, since there are some times in which the generated properties are not complete (e.g. instead of 'position held' we obtained 'position').

Case F corresponds to how many objects have been considered as Literals instead of resources. This happens with a concrete set of properties that expect this type of data as an object.

num_triplets = 154558
>>> counter
{'A': 195286, 'B': 31221, 'C': 64024, 'D': 5685, 'E': 5606, 'F': 6144}
>>> absolute
{'A': 309116, 'B': 309116, 'C': 309116, 'D': 309116, 'E': 154558, 'F': 154558}
>>> result
{'A': 63.17563633069786, 'B': 10.100091874894861, 'C': 20.711965734546254, 'D': 1.8391154129841225, 'E': 3.627117328122776, 'F': 3.975206718513438}

## 5. Discussion
